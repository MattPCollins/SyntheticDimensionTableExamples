{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a037963-f361-482d-bc03-611b0f1ea7eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd7c89e-b087-4928-9fab-afa1e9ad04fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a6ec16-c00d-44c9-a0f8-aa01af3a4bb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, IntegerType, StringType,TimestampType, DecimalType\n",
    "from pyspark.sql.functions import col, date_format\n",
    "import pyspark.sql.functions as F\n",
    "import dbldatagen as dg\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642edfc0-d3fe-498b-b87d-2a3526b22ae7",
     "showTitle": true,
     "title": "1. Random string"
    }
   },
   "outputs": [],
   "source": [
    "def create_random_string(num_characters: int) -> str:\n",
    "    return ''.join(random.choices(ascii_letters, k=num_characters))\n",
    "\n",
    "random_strings = []\n",
    "\n",
    "sample_size = 100\n",
    "list_string_lengths = [random.randrange(1, 15, 1) for i in range(sample_size)]\n",
    "\n",
    "for i in list_string_lengths:\n",
    "    print(create_random_string(i))\n",
    "    random_strings.append(create_random_string(i))\n",
    "\n",
    "print(random_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99693d1-63f8-4c58-b5a9-a7ee5aed4c7f",
     "showTitle": true,
     "title": "2.1. Static API "
    }
   },
   "outputs": [],
   "source": [
    "# Function to hit the MIT word list page, retrieving 10000 words as a list.\n",
    "def get_word_list() -> list():\n",
    "    word_site = f\"https://www.mit.edu/~ecprice/wordlist.10000\"\n",
    "    \n",
    "    response = requests.get(word_site)\n",
    "    words = response.content.splitlines()\n",
    "\n",
    "    words_array = np.array(words)\n",
    "    words_unique = np.unique(words_array)\n",
    "    words_unique = [f'{i.decode().capitalize()}' for i in words_unique ]\n",
    "    \n",
    "    return words_unique\n",
    "\n",
    "words_unique = get_word_list()\n",
    "\n",
    "sample_size = 10\n",
    "\n",
    "random.shuffle(words_unique)\n",
    "sample_words = words_unique[:sample_size]\n",
    "print(sample_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b07d34e-a4c1-4878-9a36-dac305c2fa31",
     "showTitle": true,
     "title": "2.2. Recursive API "
    }
   },
   "outputs": [],
   "source": [
    "#https://realpython.com/api-integration-in-python/\n",
    "\n",
    "def get_string_from_api(page_id: int) -> str: \n",
    "    api_url = f\"https://jsonplaceholder.typicode.com/todos/{i+1}\"\n",
    "    response = requests.get(api_url)\n",
    "    title = response.json()['title']\n",
    "    return title\n",
    "\n",
    "api_words = []\n",
    "for i in range(10):\n",
    "    api_words.append(get_string_from_api(i))\n",
    "print(api_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f9b332d-14a3-4328-a363-c17711f4ee9d",
     "showTitle": true,
     "title": "3. Webscraping"
    }
   },
   "outputs": [],
   "source": [
    "def web_scrape_japanese(page_number: int, list_japanese: list()) -> list():\n",
    "    page_url = f\"https://jisho.org/search/%23jlpt-n5%20%23words?page={page_number}\"\n",
    "    r = requests.get(page_url)\n",
    "    soup = bs(r.content, features=\"html.parser\")\n",
    "    list_classes = soup.select(\n",
    "        'div.concepts div.concept_light-representation span.text')\n",
    "    local_list_japanese = [(i.text).strip() for i in list_classes]\n",
    "    list_japanese.extend(local_list_japanese)\n",
    "    return list_japanese\n",
    "\n",
    "list_japanese = []\n",
    "page_number = 1\n",
    "sample_size = 25\n",
    "while len(list_japanese) < sample_size:\n",
    "    list_japanese = web_scrape_japanese(page_number, list_japanese)\n",
    "    page_number += 1\n",
    "print(list_japanese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa9da320-bb18-4804-aaa3-aa320998cadd",
     "showTitle": true,
     "title": "4. Faker third-party module"
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def create_faker_name():\n",
    "    return fake.name()\n",
    "\n",
    "sample_size = 100 \n",
    "list_names = [create_faker_name() for i in range(sample_size)]\n",
    "print(list_names)\n",
    "\n",
    "def faker_udf_func(list_names: list(), person_id: int):\n",
    "    return list_names[person_id]\n",
    "\n",
    "faker_udf = F.udf(faker_udf_func, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13801e47-d3ba-4924-a970-5e171ec0c895",
     "showTitle": true,
     "title": "5. ChatGPT generated function"
    }
   },
   "outputs": [],
   "source": [
    "def generate_unique_names(num_names):\n",
    "    # List of sample names\n",
    "    sample_names = ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank', 'Grace', 'Henry', 'Ivy', 'Jack']\n",
    "\n",
    "    # Ensure the requested number of names is not greater than the number of sample names\n",
    "    num_names = min(num_names, len(sample_names))\n",
    "\n",
    "    # Shuffle the sample names to get a random order\n",
    "    random.shuffle(sample_names)\n",
    "\n",
    "    # Select the first 'num_names' names\n",
    "    unique_names = sample_names[:num_names]\n",
    "\n",
    "    return unique_names\n",
    "\n",
    "# Generate 10 unique names\n",
    "result = generate_unique_names()\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5dcf1a-ebb3-439d-9b48-d620a0a2abe3",
     "showTitle": true,
     "title": "Build DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "# Example Dimension\n",
    "\n",
    "min_number = 0\n",
    "partitions_requested = 4\n",
    "\n",
    "spark.catalog.clearCache() # clear cache so that if we run multiple times to check performance, we're not relying on cache\n",
    "data_rows = 100\n",
    "\n",
    "dataspec = (dg.DataGenerator(spark, name=\"dataset\",  rows=data_rows, partitions=partitions_requested)\n",
    "            .withIdOutput()\n",
    "            .withColumn(\"Random String\", \"string\", values=random_strings, random=True)\n",
    "            .withColumn(\"Favourite MIT Word\", \"string\", values=sample_words, random=True)\n",
    "            .withColumn(\"Favourite Japanese Word\", \"string\", values=list_japanese, random=True)\n",
    "            )\n",
    "\n",
    "df = (dataspec.build().cache())\n",
    "\n",
    "df = df.withColumn(\"PersonName\", faker_udf(F.lit(list_names), df[\"id\"]))\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Random Data Generator",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
